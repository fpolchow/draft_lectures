{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outline\n",
    "\n",
    "1. Bias/Variance Tradeoff\n",
    "2. Model Complexity and Bias/Variance\n",
    "3. Tackling High Variance (Overfitting) in a model\n",
    "    * Feature Subset Selection\n",
    "    * Cross Validation\n",
    "    * Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img style= \"float:left;\" src= \"./resources/underfit.png\" width = 400><img style=\"float:right;\" src= \"./resources/overfit.png\" width = 400>\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What's wrong with these two models? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## the left one is underfit (it has a high bias)\n",
    "## the right one is overfit (it has a high variance)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Underfitting: When our model is unable to detect the \"signal\" present in our data. This is when we have High Bias.\n",
    "\n",
    "Overfitting: When our model is fit to the \"noise\" of our data rather than the \"signal.\" This is when we have High Variance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./resources/overfit_underfit.png\">\n",
    "\n",
    "image source : https://medium.com/greyatom/what-is-underfitting-and-overfitting-in-machine-learning-and-how-to-deal-with-it-6803a989c76"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bias: the difference between an estimator's expected value and the true value of the parameter being estimate\n",
    "$E[\\hat{f}(x)] - f(x)$ \n",
    "\n",
    "*Where $f(x)$ is the **true** function representing the data and $E[\\hat{f}(x)]$ is the expected predicted value of the estimated model*\n",
    "\n",
    "*In other words $\\hat{y} - y$*\n",
    "#### Variance: the variability of a model prediction for a given data point.\n",
    "$E[(\\hat{f}(x)-E[\\hat{f}(x)])^2]$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's look at the total error of a function, in this example, we'll use Mean Square Error\n",
    "\n",
    "$ MSE(X) = E[(Y - \\hat{f}(x))^2]$\n",
    "\n",
    "We can break down the components of the error further:\n",
    "#### $MSE(x) = (E[\\hat{f}(x)] - f(x))^2 + E[(\\hat{f}(x)-E[\\hat{f}(x)])^2] + \\sigma_{e}^2$\n",
    "\n",
    "$ MSE(x) = Bias^2 + Variance + Irreducible\\ Error $\n",
    "\n",
    "\n",
    "proof: http://www.cs.cmu.edu/~wcohen/10-601/bias-variance.pdf slides 9-13\n",
    "\n",
    "\n",
    "video proof explanation: https://www.youtube.com/watch?v=jiQamxz2ZcQ&t=520s "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Complexity and Bias/Variance\n",
    "As we increase our model complexity on our training set, we are more likely to be overfitting the data.\n",
    "<img src = \"./resources/bias-variance-train-test.png\" width = 500>\n",
    "\n",
    "image from https://www.learnopencv.com/bias-variance-tradeoff-in-machine-learning/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Given this plot of the errors, what is the optimal number of parameters in this model??\n",
    "<img src = \"./resources/num_parameters.png\" width = 500>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## around 5 parameters. At that point, your test error starts to increase, even though your train error\n",
    "## continues to decrease. You are overfitting if you use a parameter of \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to deal with a model that is overfit to data:\n",
    "\n",
    "* Train with more data\n",
    "* Cross Validation\n",
    "* Feature Subset Selection\n",
    "* Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train with more data\n",
    "\n",
    "Models should be trained on as large of a sample size as possible. The larger the sample, the less likely you are to fit on \"noise\" that is specific to a certain sample set.\n",
    "\n",
    "Usually, however, gathering more data is no trivial matter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Subset Selection\n",
    "\n",
    "Wrapper methods: use machine learning models with various method to determine what the optimal features in a model are. \n",
    "\n",
    "* Forward Search: Determine the most predictive single variable to regress on, repeatedly add variables until your model stops improving \n",
    "<img src=\"./resources/forward_search.png\" width =400>\n",
    "* Backward Elimination: Start with all variables and eliminate the least promising ones (the ones with the highest p-values)\n",
    "<img src= \"./resources/backward_search.png\" width = 400>\n",
    "\n",
    "These particular strategies are sometimes referred to as \"greedy\" because after you have made a decision at a particular node, you do not revisit that decision.\n",
    "\n",
    "http://jmlr.csail.mit.edu/papers/volume3/guyon03a/guyon03a.pdf page 1166\n",
    "\n",
    "http://www.biostat.jhsph.edu/~iruczins/teaching/jf/ch10.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross Validation\n",
    "To ensure that we do not overfit to the training set, we should split our data into training, validation, and test sets.\n",
    "\n",
    "#### Steps to cross validation\n",
    "1. Split your data into training and validation sets.\n",
    "2. Train models of varying complexity on the training set.\n",
    "3. Make predictions with the validation set.\n",
    "4. Choose the model that performs best on the validation set.\n",
    "\n",
    "<img src=\"./resources/train_test_split.png\" width = 500>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Cross Validation in Action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/forest.polchow/anaconda3/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mpg</th>\n",
       "      <th>cylinders</th>\n",
       "      <th>displacement</th>\n",
       "      <th>horsepower</th>\n",
       "      <th>weight</th>\n",
       "      <th>acceleration</th>\n",
       "      <th>model year</th>\n",
       "      <th>origin</th>\n",
       "      <th>car name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>18.0</td>\n",
       "      <td>8</td>\n",
       "      <td>307.0</td>\n",
       "      <td>130</td>\n",
       "      <td>3504</td>\n",
       "      <td>12.0</td>\n",
       "      <td>70</td>\n",
       "      <td>1</td>\n",
       "      <td>chevrolet chevelle malibu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>15.0</td>\n",
       "      <td>8</td>\n",
       "      <td>350.0</td>\n",
       "      <td>165</td>\n",
       "      <td>3693</td>\n",
       "      <td>11.5</td>\n",
       "      <td>70</td>\n",
       "      <td>1</td>\n",
       "      <td>buick skylark 320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>18.0</td>\n",
       "      <td>8</td>\n",
       "      <td>318.0</td>\n",
       "      <td>150</td>\n",
       "      <td>3436</td>\n",
       "      <td>11.0</td>\n",
       "      <td>70</td>\n",
       "      <td>1</td>\n",
       "      <td>plymouth satellite</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>16.0</td>\n",
       "      <td>8</td>\n",
       "      <td>304.0</td>\n",
       "      <td>150</td>\n",
       "      <td>3433</td>\n",
       "      <td>12.0</td>\n",
       "      <td>70</td>\n",
       "      <td>1</td>\n",
       "      <td>amc rebel sst</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>17.0</td>\n",
       "      <td>8</td>\n",
       "      <td>302.0</td>\n",
       "      <td>140</td>\n",
       "      <td>3449</td>\n",
       "      <td>10.5</td>\n",
       "      <td>70</td>\n",
       "      <td>1</td>\n",
       "      <td>ford torino</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    mpg  cylinders  displacement horsepower  weight  acceleration  model year  \\\n",
       "0  18.0          8         307.0        130    3504          12.0          70   \n",
       "1  15.0          8         350.0        165    3693          11.5          70   \n",
       "2  18.0          8         318.0        150    3436          11.0          70   \n",
       "3  16.0          8         304.0        150    3433          12.0          70   \n",
       "4  17.0          8         302.0        140    3449          10.5          70   \n",
       "\n",
       "   origin                   car name  \n",
       "0       1  chevrolet chevelle malibu  \n",
       "1       1          buick skylark 320  \n",
       "2       1         plymouth satellite  \n",
       "3       1              amc rebel sst  \n",
       "4       1                ford torino  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "data = pd.read_csv('./resources/auto-mpg.csv')\n",
    "data.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearRegression(copy_X=True, fit_intercept=True, n_jobs=1, normalize=False)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.linear_model import LinearRegression\n",
    "X = data['weight']\n",
    "y = data['displacement']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.2)\n",
    "lr = LinearRegression()\n",
    "lr.fit(X_train.values.reshape(-1,1),y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Training Error: 37.510117529581365\n",
      " Test Error: 37.894822290093245\n"
     ]
    }
   ],
   "source": [
    "train_predictions = lr.predict(X_train.values.reshape(-1,1))\n",
    "train_rmse = mean_squared_error(train_predictions,y_train)**0.5\n",
    "test_predictions = lr.predict(X_test.values.reshape(-1,1))\n",
    "test_rmse = mean_squared_error(test_predictions,y_test)**0.5\n",
    "print(' Training Error: {}\\n Test Error: {}'.format(train_rmse,test_rmse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##What we should see is that the test error is lower than the training error\n",
    "##What are some potential problems with this version of cross validation?\n",
    "\n",
    "#We still might be overfitting to something that is present in only the training set. As we run the previous cells\n",
    "# multiple times, we can see that we get different results for the training and test error\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### K-fold cross validation\n",
    "\n",
    "1. Split the data into k # of folds.\n",
    "2. Train your model on k-1 of the folds\n",
    "3. Test on the remaining fold.\n",
    "4. Repeat k times, and find the average score for whichever metric you are using.\n",
    "5. Once you've determined the best model, train on the entire training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1415.2821476002032\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import KFold\n",
    "folds = KFold(n_splits=5,shuffle=True)\n",
    "mse = []\n",
    "for train_idx, test_idx in folds.split(X,y):\n",
    "    lr = LinearRegression()\n",
    "    lr.fit(X[train_idx].values.reshape(-1,1),y[train_idx])\n",
    "    predictions = lr.predict(X[test_idx].values.reshape(-1,1))\n",
    "    mse.append(mean_squared_error(predictions,y[test_idx]))\n",
    "\n",
    "avg_mse = np.mean(mse)\n",
    "    \n",
    "print(avg_mse)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Other variant: Leave one out kfold Cross Validation\n",
    "\n",
    "This is the same as k-fold cross validation, except it trains on n-1 datapoints and tests on the remaining datapoint. Typically used with small sample sizes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Regularization\n",
    "\n",
    "\n",
    "Reduce the effect that each parameter has on a predictive model. We will dive more into this soon. We implement some cost function within a machine learning model. We will cover this soon..\n",
    "\n",
    "<img src = \"./resources/lasso_ridge.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resources\n",
    "\n",
    "http://scott.fortmann-roe.com/docs/BiasVariance.html\n",
    "\n",
    "https://www.youtube.com/watch?v=jiQamxz2ZcQ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
